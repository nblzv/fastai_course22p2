# Autogenerated! Edit minai_nbs/data.ipynb instead

import threading
import queue
import os
import math
import itertools

import datasets as hfds
import torch
import torchvision.transforms.functional as TF
import torchvision.io as TFIO

import minai.sampler as mins
import minai.datasets as minds

def simple_collate_func(array_of_results):
    xs = [r[0] for results in array_of_results for r in results]
    ys = [r[1] for results in array_of_results for r in results]
    return xs, ys

class HFCollate:
    def __init__(self, ds: hfds.Dataset):
        self.features = tuple(ds.features.keys())

    def __call__(self, array_of_results):
        collated = [[] for _ in range(len(self.features))]
        for result in array_of_results:
            for i, feature in enumerate(self.features):
                collated[i].extend(result[feature])
        return collated
    
    def __repr__(self):
        return f"HFCollate(features={self.features})"

WORK_TYPE_NONE = 0
WORK_TYPE_LOAD_BATCHES = 1
WORK_TYPE_GET_ITEMS = 2
WORK_TYPE_SHUTDOWN = 100

class WorkItem:
    def __init__(self, type):
        self.type = type

    def __repr__(self): return f"{self.__class__.__name__}({str(vars(self))})"

class WorkItemLoadBatches(WorkItem):
    def __init__(self, cached_iter: "Collator.CachedIter", num_batches):
        super().__init__(WORK_TYPE_LOAD_BATCHES)
        self.cached_iter = cached_iter
        self.num_batches = num_batches

class WorkItemGetItems(WorkItem):
    def __init__(self, group: "WorkGroup", indices):
        super().__init__(WORK_TYPE_GET_ITEMS)
        self.group = group
        self.indices = indices
        self.results = None

        self.group.gi_work_array.append(self)

class COPTS: # CollatorOpts
    def __init__(self,
                 sampler_iter: mins.SamplerIter = None,
                 getitem_func = None,
                 collate_func = None,
                 *, 
                 num_threads=None, 
                 cached_batch_count=1,
                 sub_batch_divisor=1.0, 
                 debug_out_queue: queue.SimpleQueue = None):
        self.sampler_iter = sampler_iter
        self.getitem_func = getitem_func
        self.collate_func = collate_func
        self.num_threads = num_threads if num_threads is not None else max(1, os.cpu_count()-1)
        self.cached_batch_count = cached_batch_count
        self.sub_batch_divisor = sub_batch_divisor
        self.debug_out_queue = debug_out_queue
        self.sub_batch_size = 0

    def finalize(self):
        if self.num_threads:
            # No divisor seems to show the best result from the very limited testing so far even with multiple threads ¯\_(ツ)_/¯
            self.sub_batch_divisor = max(1.0, self.sub_batch_divisor)
        else:
            self.cached_batch_count = 0
            self.sub_batch_divisor = 1.0
        self.sub_batch_size = math.ceil(self.sampler_iter.opts.batch_size / self.sub_batch_divisor)

        return self

class Collator:
    class CachedIter:
        def __init__(self, serial, iter, num_groups_total):
            self.serial = serial
            self.iter = iter
            self.num_groups_total = num_groups_total
            self.num_groups_requested = 0
            self.num_groups_started = 0
            self.num_groups_finished = 0
            self.lock = threading.Lock()
            self.collated_queue = queue.SimpleQueue() # Not the greatest to have a queue per CachedIter, but it simplifies the DataLoader

    def __init__(self, copts: COPTS):
        self.opts = copts.finalize()

        self.threads_spawned = False

        self.work_queue = queue.SimpleQueue()
        
        self.global_lock = threading.Lock()
        self.current_iter_serial = 0
        self.cached_iters: dict[int, Collator.CachedIter] = {}

    def start_new_iter(self):
        if not self.threads_spawned: 
            for i in range(self.opts.num_threads): threading.Thread(target=collator_threadproc, args=(i+1, self)).start()
            self.threads_spawned = True

        self.global_lock.acquire()
        
        iter_serials_to_del = []
        for iter_serial, cached_iter in self.cached_iters.items():
            if cached_iter.num_groups_requested == cached_iter.num_groups_finished:
                iter_serials_to_del.append(iter_serial)
        for iter_serial in iter_serials_to_del:
            del self.cached_iters[iter_serial]

        self.current_iter_serial += 1
        new_iter_serial = self.current_iter_serial
        if self.opts.debug_out_queue: self.opts.debug_out_queue.put(f"[0] New iter {new_iter_serial}")

        cached_iter = self.CachedIter(new_iter_serial, iter(self.opts.sampler_iter), self.opts.sampler_iter.num_batches)
        self.cached_iters[new_iter_serial] = cached_iter
        self.global_lock.release()

        self.load_batches(cached_iter, self.opts.cached_batch_count + 1)

        return cached_iter
    
    def load_batches(self, cached_iter: CachedIter, num_batches):
        queued_any = False

        cached_iter.lock.acquire()
        new_num_groups_requested = min(cached_iter.num_groups_requested + num_batches, cached_iter.num_groups_total)
        if new_num_groups_requested != cached_iter.num_groups_requested:
            to_request = new_num_groups_requested-cached_iter.num_groups_requested
            self.work_queue.put(WorkItemLoadBatches(cached_iter, to_request))
            cached_iter.num_groups_requested = new_num_groups_requested
            if self.opts.debug_out_queue: self.opts.debug_out_queue.put(f"[0] Requesting {to_request} batches for iter {iter_serial}")
            queued_any = True
        cached_iter.lock.release()

        if self.opts.num_threads == 0 and queued_any:
            collator_threadproc(0, self)

    def shutdown(self):
        if self.threads_spawned:
            for _ in range(self.opts.num_threads): self.work_queue.put(WorkItem(WORK_TYPE_SHUTDOWN))

class WorkGroup:
    def __init__(self, cached_iter: "Collator.CachedIter", batch_serial, num_total):
        self.cached_iter = cached_iter
        self.batch_serial = batch_serial
        self.num_total = num_total
        self.gi_work_array: list[WorkItemGetItems] = []

        self.num_done_lock = threading.Lock()
        self.num_done = 0

    def __repr__(self): return f"{self.__class__.__name__}(iter_serial={self.iter_serial}, batch_serial={self.batch_serial}, "\
                                    f"num_done={self.num_done}, num_total={self.num_total})"

class CollatedResult:
    def __init__(self, batch_serial, result):
        self.batch_serial = batch_serial
        self.result = result

    def __repr__(self): return f"{self.__class__.__name__}({str(vars(self))})"

def collator_threadproc(thread_id: int, ctx: Collator):
    work_queue = ctx.work_queue
    global_lock = ctx.global_lock
    sub_batch_size = ctx.opts.sub_batch_size
    cached_iters = ctx.cached_iters
    getitem_func = ctx.opts.getitem_func
    collate_func = ctx.opts.collate_func
    debug_out_queue = ctx.opts.debug_out_queue

    if debug_out_queue: debug_out_queue.put(f"[{thread_id}] Started")

    while 1:
        work: WorkItem = work_queue.get()
        if debug_out_queue: debug_out_queue.put(f"[{thread_id}] Got {work}")
        work_type = work.type
        
        if work_type == WORK_TYPE_LOAD_BATCHES:
            lb_work: WorkItemLoadBatches = work
            
            cached_iter = lb_work.cached_iter
            
            cached_iter.lock.acquire()
            read_num_groups = cached_iter.num_groups_started
            
            cached_iter.num_groups_started += lb_work.num_batches
            array_of_batch_indices = list(itertools.islice(cached_iter.iter, lb_work.num_batches))
            cached_iter.lock.release()

            batches_spawned = 0
            for batch_indices in array_of_batch_indices:
                sub_batches = mins.chunkify(batch_indices, sub_batch_size)
                assert len(sub_batches)

                batch_serial = read_num_groups + batches_spawned
                work_group = WorkGroup(lb_work.cached_iter, batch_serial + 1, len(sub_batches))
                for sub_batch in sub_batches:
                    work_queue.put(WorkItemGetItems(work_group, sub_batch))
                    if debug_out_queue: debug_out_queue.put(f"[{thread_id}] Queued {work_group.gi_work_array[-1]}")

                batches_spawned += 1


        elif work_type == WORK_TYPE_GET_ITEMS:
            gi_work: WorkItemGetItems = work
            work_group = gi_work.group

            gi_work.results = getitem_func(gi_work.indices)

            work_group.num_done_lock.acquire()
            work_group.num_done += 1
            work_group.num_done_lock.release()
            assert work_group.num_done <= work_group.num_total
            
            if work_group.num_done == work_group.num_total:
                assert work_group.num_done == len(work_group.gi_work_array)
                if debug_out_queue: debug_out_queue.put(f"[{thread_id}] Completed group {work_group}")

                cached_iter = work_group.cached_iter
                
                cached_iter.lock.acquire()
                cached_iter.num_groups_finished += 1
                is_last_group = cached_iter.num_groups_finished == cached_iter.num_groups_total
                cached_iter.lock.release()

                collated = collate_func([gi_work.results for gi_work in work_group.gi_work_array])
                cached_iter.collated_queue.put(CollatedResult(work_group.batch_serial, collated))

                if is_last_group:
                    if debug_out_queue: debug_out_queue.put(f"[{thread_id}] Completed iter {cached_iter.serial}")
                    cached_iter.collated_queue.put(CollatedResult(0, None))

                if thread_id == 0:
                    break
                

        elif work_type == WORK_TYPE_SHUTDOWN:
            break

        else: assert False

    if debug_out_queue: debug_out_queue.put(f"[{thread_id}] Exiting")


class DataLoaderIter:
    def __init__(self, collator, cached_iter: Collator.CachedIter, collated_queue):
        self.collator = collator
        self.cached_iter = cached_iter
        self.collated_queue = collated_queue
        
        self.buffer: list[CollatedResult] = []
        self.next_batch_index = 1
    
    def __iter__(self):
        while 1:
            collated: CollatedResult = self.collated_queue.get()
            if not collated.batch_serial:
                assert not self.buffer # batch_serial == 0 should strictly come last
                break

            self.buffer.append(collated)

            found_i = next((i for i, x in enumerate(self.buffer) if x.batch_serial == self.next_batch_index), -1)
            if found_i != -1:
                to_yield = self.buffer[found_i].result
                self.buffer[found_i] = self.buffer[-1]
                self.buffer.pop()
                self.next_batch_index += 1
                
                yield to_yield
                self.collator.load_batches(self.cached_iter, 1)

class DataLoader:
    def __init__(self, ds, copts: COPTS):
        self.ds = ds
        self.collator = Collator(copts)

    def __del__(self):
        self.collator.shutdown()
    
    def __iter__(self):
        cached_iter = self.collator.start_new_iter()
        return iter(DataLoaderIter(self.collator, cached_iter, cached_iter.collated_queue))
    
    @classmethod
    def simple(cls, simple_ds: minds.SimpleDataset, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):
        sampler_iter_opts = sampler_iter_opts or mins.SIO()
        collator_opts = collator_opts or COPTS()

        collator_opts.sampler_iter = mins.Sampler(len(simple_ds)).iter(sampler_iter_opts)
        collator_opts.getitem_func = simple_ds.__getitem__
        collator_opts.collate_func = simple_collate_func
        return cls(simple_ds, collator_opts)

    @classmethod
    def hf(cls, hf_ds: hfds.Dataset, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):
        assert type(hf_ds) is hfds.Dataset, f"Dataset expected, not {type(hf_ds).__name__}"
        sampler_iter_opts = sampler_iter_opts or mins.SIO()
        collator_opts = collator_opts or COPTS()

        collator_opts.sampler_iter = mins.Sampler(len(hf_ds)).iter(sampler_iter_opts)
        collator_opts.getitem_func = hf_ds.__getitem__
        collator_opts.collate_func = HFCollate(hf_ds).__call__
        return cls(hf_ds, collator_opts)

class HFTransform:
    def __init__(self, features, transform, **extra_args):
        assert type(features) is hfds.features.features.Features
        self.features = tuple(features)
        self.transform = transform

        for k, v in extra_args.items():
            super().__setattr__(k ,v)

    def __call__(self, results):
        return self.transform(self, results)
    
    def __repr__(self):
        return f"HFTransform(features={list(self.features)})"

    @classmethod
    def ff_img_to_tensor(cls, features): # first_feature
        def tf(ctx: HFTransform, results):
            xs = results[ctx.features[0]]
            for i in range(len(xs)):
                xs[i] = TF.to_tensor(xs[i])
            return results
        
        return cls(features, tf)
    
    @classmethod
    def ff_img_decode_to_tensor(cls, features, half=False): # first_feature
        def tf(ctx: HFTransform, results):
            xs = results[ctx.features[0]]
            for i in range(len(xs)):
                raw = torch.frombuffer(xs[i]["bytes"], dtype=torch.uint8)
                decoded = TFIO.decode_image(raw)
                if ctx.half: decoded = decode.half()
                else: decoded = decoded.float()
                xs[i] = decoded / 255.0
            return results
        
        return cls(features, tf, half=half)

def first(iterable):
    return next(iter(iterable))

def first_value(iterable):
    return next(iter(iterable.values()))

class DataLoaderDict(dict):
    def __init__(self, dataloaders_dict: dict[str, DataLoader]):
        super().__init__(dataloaders_dict)

    def __getitem__(self, key) -> DataLoader:
        return super().__getitem__(key)
        
    def __repr__(self):
        return f"DataLoaders({super().__repr__()})"

    @classmethod
    def hf(cls, dsd: hfds.DatasetDict, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):
        dls = {k: DataLoader.hf(dsd[k], sampler_iter_opts, collator_opts) for k in dsd}
        return cls(dls)

