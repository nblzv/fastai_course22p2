{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import datasets as hfds\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.io as TFIO\n",
    "\n",
    "import minai.sampler as mins\n",
    "import minai.datasets as minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "def simple_collate_func(array_of_results):\n",
    "    xs = [r[0] for results in array_of_results for r in results]\n",
    "    ys = [r[1] for results in array_of_results for r in results]\n",
    "    return xs, ys\n",
    "\n",
    "class HFCollate:\n",
    "    def __init__(self, ds: hfds.Dataset):\n",
    "        self.features = tuple(ds.features.keys())\n",
    "\n",
    "    def __call__(self, array_of_results):\n",
    "        collated = [[] for _ in range(len(self.features))]\n",
    "        for result in array_of_results:\n",
    "            for i, feature in enumerate(self.features):\n",
    "                collated[i].extend(result[feature])\n",
    "        return collated\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"HFCollate(features={self.features})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "WORK_TYPE_NONE = 0\n",
    "WORK_TYPE_LOAD_BATCHES = 1\n",
    "WORK_TYPE_GET_ITEMS = 2\n",
    "WORK_TYPE_SHUTDOWN = 100\n",
    "\n",
    "class WorkItem:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}({str(vars(self))})\"\n",
    "\n",
    "class WorkItemLoadBatches(WorkItem):\n",
    "    def __init__(self, cached_iter: \"Collator.CachedIter\", num_batches):\n",
    "        super().__init__(WORK_TYPE_LOAD_BATCHES)\n",
    "        self.cached_iter = cached_iter\n",
    "        self.num_batches = num_batches\n",
    "\n",
    "class WorkItemGetItems(WorkItem):\n",
    "    def __init__(self, group: \"WorkGroup\", indices):\n",
    "        super().__init__(WORK_TYPE_GET_ITEMS)\n",
    "        self.group = group\n",
    "        self.indices = indices\n",
    "        self.results = None\n",
    "\n",
    "        self.group.gi_work_array.append(self)\n",
    "\n",
    "class COPTS: # CollatorOpts\n",
    "    def __init__(self,\n",
    "                 sampler_iter: mins.SamplerIter = None,\n",
    "                 getitem_func = None,\n",
    "                 collate_func = None,\n",
    "                 *, \n",
    "                 num_threads=None, \n",
    "                 cached_batch_count=1,\n",
    "                 sub_batch_divisor=1.0, \n",
    "                 debug_out_queue: queue.SimpleQueue = None):\n",
    "        self.sampler_iter = sampler_iter\n",
    "        self.getitem_func = getitem_func\n",
    "        self.collate_func = collate_func\n",
    "        self.num_threads = num_threads if num_threads is not None else max(1, os.cpu_count()-1)\n",
    "        self.cached_batch_count = cached_batch_count\n",
    "        self.sub_batch_divisor = sub_batch_divisor\n",
    "        self.debug_out_queue = debug_out_queue\n",
    "        self.sub_batch_size = 0\n",
    "\n",
    "    def finalize(self):\n",
    "        if self.num_threads:\n",
    "            # No divisor seems to show the best result from the very limited testing so far even with multiple threads ¯\\_(ツ)_/¯\n",
    "            self.sub_batch_divisor = max(1.0, self.sub_batch_divisor)\n",
    "        else:\n",
    "            self.cached_batch_count = 0\n",
    "            self.sub_batch_divisor = 1.0\n",
    "        self.sub_batch_size = math.ceil(self.sampler_iter.opts.batch_size / self.sub_batch_divisor)\n",
    "\n",
    "        return self\n",
    "\n",
    "class Collator:\n",
    "    class CachedIter:\n",
    "        def __init__(self, serial, iter, num_groups_total):\n",
    "            self.serial = serial\n",
    "            self.iter = iter\n",
    "            self.num_groups_total = num_groups_total\n",
    "            self.num_groups_requested = 0\n",
    "            self.num_groups_started = 0\n",
    "            self.num_groups_finished = 0\n",
    "            self.lock = threading.Lock()\n",
    "            self.collated_queue = queue.SimpleQueue() # Not the greatest to have a queue per CachedIter, but it simplifies the DataLoader\n",
    "\n",
    "    def __init__(self, copts: COPTS):\n",
    "        self.opts = copts.finalize()\n",
    "\n",
    "        self.threads_spawned = False\n",
    "\n",
    "        self.work_queue = queue.SimpleQueue()\n",
    "        \n",
    "        self.global_lock = threading.Lock()\n",
    "        self.current_iter_serial = 0\n",
    "        self.cached_iters: dict[int, Collator.CachedIter] = {}\n",
    "\n",
    "    def start_new_iter(self):\n",
    "        if not self.threads_spawned: \n",
    "            for i in range(self.opts.num_threads): threading.Thread(target=collator_threadproc, args=(i+1, self)).start()\n",
    "            self.threads_spawned = True\n",
    "\n",
    "        self.global_lock.acquire()\n",
    "        \n",
    "        iter_serials_to_del = []\n",
    "        for iter_serial, cached_iter in self.cached_iters.items():\n",
    "            if cached_iter.num_groups_requested == cached_iter.num_groups_finished:\n",
    "                iter_serials_to_del.append(iter_serial)\n",
    "        for iter_serial in iter_serials_to_del:\n",
    "            del self.cached_iters[iter_serial]\n",
    "\n",
    "        self.current_iter_serial += 1\n",
    "        new_iter_serial = self.current_iter_serial\n",
    "        if self.opts.debug_out_queue: self.opts.debug_out_queue.put(f\"[0] New iter {new_iter_serial}\")\n",
    "\n",
    "        cached_iter = self.CachedIter(new_iter_serial, iter(self.opts.sampler_iter), self.opts.sampler_iter.num_batches)\n",
    "        self.cached_iters[new_iter_serial] = cached_iter\n",
    "        self.global_lock.release()\n",
    "\n",
    "        self.load_batches(cached_iter, self.opts.cached_batch_count + 1)\n",
    "\n",
    "        return cached_iter\n",
    "    \n",
    "    def load_batches(self, cached_iter: CachedIter, num_batches):\n",
    "        queued_any = False\n",
    "\n",
    "        cached_iter.lock.acquire()\n",
    "        new_num_groups_requested = min(cached_iter.num_groups_requested + num_batches, cached_iter.num_groups_total)\n",
    "        if new_num_groups_requested != cached_iter.num_groups_requested:\n",
    "            to_request = new_num_groups_requested-cached_iter.num_groups_requested\n",
    "            self.work_queue.put(WorkItemLoadBatches(cached_iter, to_request))\n",
    "            cached_iter.num_groups_requested = new_num_groups_requested\n",
    "            if self.opts.debug_out_queue: self.opts.debug_out_queue.put(f\"[0] Requesting {to_request} batches for iter {iter_serial}\")\n",
    "            queued_any = True\n",
    "        cached_iter.lock.release()\n",
    "\n",
    "        if self.opts.num_threads == 0 and queued_any:\n",
    "            collator_threadproc(0, self)\n",
    "\n",
    "    def shutdown(self):\n",
    "        if self.threads_spawned:\n",
    "            for _ in range(self.opts.num_threads): self.work_queue.put(WorkItem(WORK_TYPE_SHUTDOWN))\n",
    "\n",
    "class WorkGroup:\n",
    "    def __init__(self, cached_iter: \"Collator.CachedIter\", batch_serial, num_total):\n",
    "        self.cached_iter = cached_iter\n",
    "        self.batch_serial = batch_serial\n",
    "        self.num_total = num_total\n",
    "        self.gi_work_array: list[WorkItemGetItems] = []\n",
    "\n",
    "        self.num_done_lock = threading.Lock()\n",
    "        self.num_done = 0\n",
    "\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}(iter_serial={self.iter_serial}, batch_serial={self.batch_serial}, \"\\\n",
    "                                    f\"num_done={self.num_done}, num_total={self.num_total})\"\n",
    "\n",
    "class CollatedResult:\n",
    "    def __init__(self, batch_serial, result):\n",
    "        self.batch_serial = batch_serial\n",
    "        self.result = result\n",
    "\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}({str(vars(self))})\"\n",
    "\n",
    "def collator_threadproc(thread_id: int, ctx: Collator):\n",
    "    work_queue = ctx.work_queue\n",
    "    global_lock = ctx.global_lock\n",
    "    sub_batch_size = ctx.opts.sub_batch_size\n",
    "    cached_iters = ctx.cached_iters\n",
    "    getitem_func = ctx.opts.getitem_func\n",
    "    collate_func = ctx.opts.collate_func\n",
    "    debug_out_queue = ctx.opts.debug_out_queue\n",
    "\n",
    "    if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Started\")\n",
    "\n",
    "    while 1:\n",
    "        work: WorkItem = work_queue.get()\n",
    "        if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Got {work}\")\n",
    "        work_type = work.type\n",
    "        \n",
    "        if work_type == WORK_TYPE_LOAD_BATCHES:\n",
    "            lb_work: WorkItemLoadBatches = work\n",
    "            \n",
    "            cached_iter = lb_work.cached_iter\n",
    "            \n",
    "            cached_iter.lock.acquire()\n",
    "            read_num_groups = cached_iter.num_groups_started\n",
    "            \n",
    "            cached_iter.num_groups_started += lb_work.num_batches\n",
    "            array_of_batch_indices = list(itertools.islice(cached_iter.iter, lb_work.num_batches))\n",
    "            cached_iter.lock.release()\n",
    "\n",
    "            batches_spawned = 0\n",
    "            for batch_indices in array_of_batch_indices:\n",
    "                sub_batches = mins.chunkify(batch_indices, sub_batch_size)\n",
    "                assert len(sub_batches)\n",
    "\n",
    "                batch_serial = read_num_groups + batches_spawned\n",
    "                work_group = WorkGroup(lb_work.cached_iter, batch_serial + 1, len(sub_batches))\n",
    "                for sub_batch in sub_batches:\n",
    "                    work_queue.put(WorkItemGetItems(work_group, sub_batch))\n",
    "                    if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Queued {work_group.gi_work_array[-1]}\")\n",
    "\n",
    "                batches_spawned += 1\n",
    "\n",
    "\n",
    "        elif work_type == WORK_TYPE_GET_ITEMS:\n",
    "            gi_work: WorkItemGetItems = work\n",
    "            work_group = gi_work.group\n",
    "\n",
    "            gi_work.results = getitem_func(gi_work.indices)\n",
    "\n",
    "            work_group.num_done_lock.acquire()\n",
    "            work_group.num_done += 1\n",
    "            work_group.num_done_lock.release()\n",
    "            assert work_group.num_done <= work_group.num_total\n",
    "            \n",
    "            if work_group.num_done == work_group.num_total:\n",
    "                assert work_group.num_done == len(work_group.gi_work_array)\n",
    "                if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Completed group {work_group}\")\n",
    "\n",
    "                cached_iter = work_group.cached_iter\n",
    "                \n",
    "                cached_iter.lock.acquire()\n",
    "                cached_iter.num_groups_finished += 1\n",
    "                is_last_group = cached_iter.num_groups_finished == cached_iter.num_groups_total\n",
    "                cached_iter.lock.release()\n",
    "\n",
    "                collated = collate_func([gi_work.results for gi_work in work_group.gi_work_array])\n",
    "                cached_iter.collated_queue.put(CollatedResult(work_group.batch_serial, collated))\n",
    "\n",
    "                if is_last_group:\n",
    "                    if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Completed iter {cached_iter.serial}\")\n",
    "                    cached_iter.collated_queue.put(CollatedResult(0, None))\n",
    "\n",
    "                if thread_id == 0:\n",
    "                    break\n",
    "                \n",
    "\n",
    "        elif work_type == WORK_TYPE_SHUTDOWN:\n",
    "            break\n",
    "\n",
    "        else: assert False\n",
    "\n",
    "    if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Exiting\")\n",
    "\n",
    "\n",
    "class DataLoaderIter:\n",
    "    def __init__(self, collator, cached_iter: Collator.CachedIter, collated_queue):\n",
    "        self.collator = collator\n",
    "        self.cached_iter = cached_iter\n",
    "        self.collated_queue = collated_queue\n",
    "        \n",
    "        self.buffer: list[CollatedResult] = []\n",
    "        self.next_batch_index = 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while 1:\n",
    "            collated: CollatedResult = self.collated_queue.get()\n",
    "            if not collated.batch_serial:\n",
    "                assert not self.buffer # batch_serial == 0 should strictly come last\n",
    "                break\n",
    "\n",
    "            self.buffer.append(collated)\n",
    "\n",
    "            found_i = next((i for i, x in enumerate(self.buffer) if x.batch_serial == self.next_batch_index), -1)\n",
    "            if found_i != -1:\n",
    "                to_yield = self.buffer[found_i].result\n",
    "                self.buffer[found_i] = self.buffer[-1]\n",
    "                self.buffer.pop()\n",
    "                self.next_batch_index += 1\n",
    "                \n",
    "                yield to_yield\n",
    "                self.collator.load_batches(self.cached_iter, 1)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, ds, copts: COPTS):\n",
    "        self.ds = ds\n",
    "        self.collator = Collator(copts)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.collator.shutdown()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        cached_iter = self.collator.start_new_iter()\n",
    "        return iter(DataLoaderIter(self.collator, cached_iter, cached_iter.collated_queue))\n",
    "    \n",
    "    @classmethod\n",
    "    def simple(cls, simple_ds: minds.SimpleDataset, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):\n",
    "        sampler_iter_opts = sampler_iter_opts or mins.SIO()\n",
    "        collator_opts = collator_opts or COPTS()\n",
    "\n",
    "        collator_opts.sampler_iter = mins.Sampler(len(simple_ds)).iter(sampler_iter_opts)\n",
    "        collator_opts.getitem_func = simple_ds.__getitem__\n",
    "        collator_opts.collate_func = simple_collate_func\n",
    "        return cls(simple_ds, collator_opts)\n",
    "\n",
    "    @classmethod\n",
    "    def hf(cls, hf_ds: hfds.Dataset, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):\n",
    "        assert type(hf_ds) is hfds.Dataset, f\"Dataset expected, not {type(hf_ds).__name__}\"\n",
    "        sampler_iter_opts = sampler_iter_opts or mins.SIO()\n",
    "        collator_opts = collator_opts or COPTS()\n",
    "\n",
    "        collator_opts.sampler_iter = mins.Sampler(len(hf_ds)).iter(sampler_iter_opts)\n",
    "        collator_opts.getitem_func = hf_ds.__getitem__\n",
    "        collator_opts.collate_func = HFCollate(hf_ds).__call__\n",
    "        return cls(hf_ds, collator_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ([0, 1, 2, 3, 4], [0, -1, -2, -3, -4])\n",
      "2 ([0, 1, 2, 3, 4], [0, -1, -2, -3, -4])\n",
      "1 ([5, 6, 7, 8, 9], [-5, -6, -7, -8, -9])\n",
      "1 ([10, 3, 3, 6, 8], [-10, -3, -3, -6, -8])\n",
      "2 ([5, 6, 7, 8, 9], [-5, -6, -7, -8, -9])\n",
      "2 ([10, 10, 6, 5, 7], [-10, -10, -6, -5, -7])\n",
      "\n",
      "[0, 1, 2, 3, 4] [0, -1, -2, -3, -4]\n",
      "[5, 6, 7, 8, 9] [-5, -6, -7, -8, -9]\n",
      "[10, 3, 8, 1, 4] [-10, -3, -8, -1, -4]\n",
      "agane\n",
      "[0, 1, 2, 3, 4] [0, -1, -2, -3, -4]\n",
      "[5, 6, 7, 8, 9] [-5, -6, -7, -8, -9]\n",
      "[10, 6, 9, 7, 2] [-10, -6, -9, -7, -2]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def getitem_func(indices):\n",
    "    #time.sleep(random.random())\n",
    "    return [[i, -i] for i in indices]\n",
    "\n",
    "dl = DataLoader(None, COPTS(mins.Sampler(11).iter(mins.SIO(5)), getitem_func, simple_collate_func, num_threads=4, cached_batch_count=1))\n",
    "\n",
    "it1 = iter(dl)\n",
    "it2 = iter(dl)\n",
    "print(1, next(it1))\n",
    "print(2, next(it2))\n",
    "print(1, next(it1))\n",
    "print(1, next(it1))\n",
    "print(2, next(it2))\n",
    "print(2, next(it2))\n",
    "print()\n",
    "\n",
    "for xb, yb in dl:\n",
    "    print(xb, yb)\n",
    "print(\"agane\")\n",
    "for xb, yb in dl:\n",
    "    print(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SimpleDataset(len=100, xs=int, ys=int),\n",
       " {'opts': <__main__.COPTS at 0x7f31b4c5e1d0>,\n",
       "  'threads_spawned': False,\n",
       "  'work_queue': <_queue.SimpleQueue at 0x7f31b4996520>,\n",
       "  'global_lock': <unlocked _thread.lock object at 0x7f31b4c5e300>,\n",
       "  'current_iter_serial': 0,\n",
       "  'cached_iters': {}},\n",
       " {'sampler_iter': <minai.sampler.SamplerIter at 0x7f31b4c5e7d0>,\n",
       "  'getitem_func': <bound method SimpleDataset.__getitem__ of SimpleDataset(len=100, xs=int, ys=int)>,\n",
       "  'collate_func': <function __main__.simple_collate_func(array_of_results)>,\n",
       "  'num_threads': 14,\n",
       "  'cached_batch_count': 1,\n",
       "  'sub_batch_divisor': 1.0,\n",
       "  'debug_out_queue': None,\n",
       "  'sub_batch_size': 16})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = minds.SimpleDataset(list(range(100)), list(range(0, -100, -1)))\n",
    "dl = DataLoader.simple(ds, mins.SIO(16))\n",
    "ds, vars(dl.collator), vars(dl.collator.opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15])\n",
      "([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31])\n",
      "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15])\n",
      "([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31])\n"
     ]
    }
   ],
   "source": [
    "it = iter(dl)\n",
    "print(next(it))\n",
    "print(next(it))\n",
    "\n",
    "it = iter(dl)\n",
    "print(next(it))\n",
    "print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fashion_mnist (/home/nblzv/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890c15ab2bfd45f1967ddf57d36f9cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsd = minds.hf_load(minds.HF_DATASETS.FASHION_MNIST)\n",
    "dst = dsd[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFCollate(features=('image', 'label'))\n",
      "[[<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F3190B4D5D0>, <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F3190B4D6D0>, <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F3190124FD0>], [9, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "c = HFCollate(dst)\n",
    "print(c)\n",
    "print(c([dst[[0, 1]], dst[[2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15]\n",
      "[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31] [-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31]\n",
      "[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47] [-32, -33, -34, -35, -36, -37, -38, -39, -40, -41, -42, -43, -44, -45, -46, -47]\n",
      "[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63] [-48, -49, -50, -51, -52, -53, -54, -55, -56, -57, -58, -59, -60, -61, -62, -63]\n",
      "[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79] [-64, -65, -66, -67, -68, -69, -70, -71, -72, -73, -74, -75, -76, -77, -78, -79]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95] [-80, -81, -82, -83, -84, -85, -86, -87, -88, -89, -90, -91, -92, -93, -94, -95]\n",
      "[96, 97, 98, 99, 79, 5, 86, 3, 86, 35, 37, 11, 84, 65, 9, 55] [-96, -97, -98, -99, -79, -5, -86, -3, -86, -35, -37, -11, -84, -65, -9, -55]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15]\n",
      "[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31] [-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31]\n",
      "[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47] [-32, -33, -34, -35, -36, -37, -38, -39, -40, -41, -42, -43, -44, -45, -46, -47]\n",
      "[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63] [-48, -49, -50, -51, -52, -53, -54, -55, -56, -57, -58, -59, -60, -61, -62, -63]\n",
      "[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79] [-64, -65, -66, -67, -68, -69, -70, -71, -72, -73, -74, -75, -76, -77, -78, -79]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95] [-80, -81, -82, -83, -84, -85, -86, -87, -88, -89, -90, -91, -92, -93, -94, -95]\n",
      "[96, 97, 98, 99, 41, 10, 62, 34, 86, 29, 36, 18, 21, 0, 50, 84] [-96, -97, -98, -99, -41, -10, -62, -34, -86, -29, -36, -18, -21, 0, -50, -84]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    for xs, ys in dl:\n",
    "        print(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>],\n",
       " [9, 0, 0, 3, 0, 2, 7, 2, 5]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader.hf(dst, mins.SIO(9, False))\n",
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "class HFTransform:\n",
    "    def __init__(self, features, transform, **extra_args):\n",
    "        assert type(features) is hfds.features.features.Features\n",
    "        self.features = tuple(features)\n",
    "        self.transform = transform\n",
    "\n",
    "        for k, v in extra_args.items():\n",
    "            super().__setattr__(k ,v)\n",
    "\n",
    "    def __call__(self, results):\n",
    "        return self.transform(self, results)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"HFTransform(features={list(self.features)})\"\n",
    "\n",
    "    @classmethod\n",
    "    def ff_img_to_tensor(cls, features): # first_feature\n",
    "        def tf(ctx: HFTransform, results):\n",
    "            xs = results[ctx.features[0]]\n",
    "            for i in range(len(xs)):\n",
    "                xs[i] = TF.to_tensor(xs[i])\n",
    "            return results\n",
    "        \n",
    "        return cls(features, tf)\n",
    "    \n",
    "    @classmethod\n",
    "    def ff_img_decode_to_tensor(cls, features, half=False): # first_feature\n",
    "        def tf(ctx: HFTransform, results):\n",
    "            xs = results[ctx.features[0]]\n",
    "            for i in range(len(xs)):\n",
    "                raw = torch.frombuffer(xs[i][\"bytes\"], dtype=torch.uint8)\n",
    "                decoded = TFIO.decode_image(raw)\n",
    "                if ctx.half: decoded = decode.half()\n",
    "                else: decoded = decoded.float()\n",
    "                xs[i] = decoded / 255.0\n",
    "            return results\n",
    "        \n",
    "        return cls(features, tf, half=half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "def first(iterable):\n",
    "    return next(iter(iterable))\n",
    "\n",
    "def first_value(iterable):\n",
    "    return next(iter(iterable.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_tensor_transform(ctx, results):\n",
    "    xs = results[ctx.features[0]]\n",
    "    for i in range(len(xs)):\n",
    "        xs[i] = TF.to_tensor(xs[i])\n",
    "    return results\n",
    "\n",
    "thds = dst.with_transform(HFTransform(dst.features, to_tensor_transform))\n",
    "dl = DataLoader.hf(thds, mins.SIO(5, False))\n",
    "first(dl)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefetch 0\n",
      "13.1 ms ± 614 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "20.3 ms ± 711 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "27.7 ms ± 890 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "prefetch 1\n",
      "35.3 ms ± 1.75 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "59 ms ± 14.5 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "56.2 ms ± 10.2 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "tdst = dst.with_transform(HFTransform.ff_img_to_tensor(dst.features))\n",
    "def bench(cached_batch_count, next_count):\n",
    "    dl = DataLoader.hf(tdst, mins.SIO(64), collator_opts=COPTS(cached_batch_count=cached_batch_count))\n",
    "    it = iter(dl)\n",
    "    for _ in range(next_count): next(it)\n",
    "print(\"prefetch 0\")\n",
    "%timeit -r 10 -n 1 bench(0, 1);\n",
    "%timeit -r 10 -n 1 bench(0, 2);\n",
    "%timeit -r 10 -n 1 bench(0, 3);\n",
    "print(\"prefetch 1\")\n",
    "%timeit -r 10 -n 1 bench(1, 1); # For some reason this shows as slower when it is in fact pretty much the same time for the first batch to complete\n",
    "%timeit -r 10 -n 1 bench(1, 2);\n",
    "%timeit -r 10 -n 1 bench(1, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefetch 0\n",
      "8.55 ms ± 551 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "12.5 ms ± 480 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "16.6 ms ± 476 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "prefetch 1\n",
      "16.7 ms ± 837 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "28.4 ms ± 6.68 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "26.8 ms ± 2.43 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "tdst = dst.with_transform(HFTransform.ff_img_decode_to_tensor(dst.features)).cast_column(\"image\", hfds.Image(decode=False))\n",
    "def bench(cached_batch_count, next_count):\n",
    "    dl = DataLoader.hf(tdst, mins.SIO(64), collator_opts=COPTS(cached_batch_count=cached_batch_count))\n",
    "    it = iter(dl)\n",
    "    for _ in range(next_count): \n",
    "        next(it)\n",
    "print(\"prefetch 0\")\n",
    "%timeit -r 10 -n 1 bench(0, 1);\n",
    "%timeit -r 10 -n 1 bench(0, 2);\n",
    "%timeit -r 10 -n 1 bench(0, 3);\n",
    "print(\"prefetch 1\")\n",
    "%timeit -r 10 -n 1 bench(1, 1); # For some reason this shows as slower when it is in fact pretty much the same time for the first batch to complete\n",
    "%timeit -r 10 -n 1 bench(1, 2);\n",
    "%timeit -r 10 -n 1 bench(1, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "class DataLoaderDict(dict):\n",
    "    def __init__(self, dataloaders_dict: dict[str, DataLoader]):\n",
    "        super().__init__(dataloaders_dict)\n",
    "\n",
    "    def __getitem__(self, key) -> DataLoader:\n",
    "        return super().__getitem__(key)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"DataLoaders({super().__repr__()})\"\n",
    "\n",
    "    @classmethod\n",
    "    def hf(cls, dsd: hfds.DatasetDict, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):\n",
    "        dls = {k: DataLoader.hf(dsd[k], sampler_iter_opts, collator_opts) for k in dsd}\n",
    "        return cls(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoaders({'train': <__main__.DataLoader object at 0x7f3164326ad0>, 'test': <__main__.DataLoader object at 0x7f31674ee2d0>})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaderDict.hf(dsd)\n",
    "dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing minai_nbs/datasets.ipynb -> minai/minai/datasets.py  |  same contents, skipping, took 0.001s\n",
      "Processing minai_nbs/sampler.ipynb -> minai/minai/sampler.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/setup+template.py -> minai/setup.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/__init__+template.py -> minai/minai/__init__.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/plot.ipynb -> minai/minai/plot.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/mintils.py -> minai/minai/mintils.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/data.ipynb -> minai/minai/data.py  |  same contents, skipping, took 0.000s\n",
      "\n",
      "All done... took 0.003s\n",
      "  lib_name: minai\n",
      "  author: nblzv\n",
      "  version: 0.1.1\n"
     ]
    }
   ],
   "source": [
    "import z_export\n",
    "z_export.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
