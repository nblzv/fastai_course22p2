{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "import threading\n",
    "import queue\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import datasets as hfds\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "import torchvision.io as TFIO\n",
    "\n",
    "import minai.sampler as mins\n",
    "import minai.datasets as minds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "def simple_collate_func(array_of_results):\n",
    "    xs = [r[0] for results in array_of_results for r in results]\n",
    "    ys = [r[1] for results in array_of_results for r in results]\n",
    "    return xs, ys\n",
    "\n",
    "class HFCollate:\n",
    "    def __init__(self, ds: hfds.Dataset):\n",
    "        self.features = tuple(ds.features.keys())\n",
    "\n",
    "    def __call__(self, array_of_results):\n",
    "        collated = [[] for _ in range(len(self.features))]\n",
    "        for result in array_of_results:\n",
    "            for i, feature in enumerate(self.features):\n",
    "                collated[i].extend(result[feature])\n",
    "        return collated\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"HFCollate(features={self.features})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "WORK_TYPE_NONE = 0\n",
    "WORK_TYPE_LOAD_BATCHES = 1\n",
    "WORK_TYPE_GET_ITEMS = 2\n",
    "WORK_TYPE_SHUTDOWN = 100\n",
    "\n",
    "class WorkItem:\n",
    "    def __init__(self, type):\n",
    "        self.type = type\n",
    "\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}({str(vars(self))})\"\n",
    "\n",
    "class WorkItemLoadBatches(WorkItem):\n",
    "    def __init__(self, iter_serial, num_batches):\n",
    "        super().__init__(WORK_TYPE_LOAD_BATCHES)\n",
    "        self.iter_serial = iter_serial\n",
    "        self.num_batches = num_batches\n",
    "\n",
    "class WorkItemGetItems(WorkItem):\n",
    "    def __init__(self, group: \"WorkGroup\", indices):\n",
    "        super().__init__(WORK_TYPE_GET_ITEMS)\n",
    "        self.group = group\n",
    "        self.indices = indices\n",
    "        self.results = None\n",
    "\n",
    "        self.group.gi_work_array.append(self)\n",
    "\n",
    "class COPTS: # CollatorOpts\n",
    "    def __init__(self,\n",
    "                 sampler_iter: mins.SamplerIter = None,\n",
    "                 getitem_func = None,\n",
    "                 collate_func = None,\n",
    "                 *, \n",
    "                 num_threads=None, \n",
    "                 cached_batch_count=1,\n",
    "                 sub_batch_divisor=1.0, \n",
    "                 debug_out_queue: queue.SimpleQueue = None):\n",
    "        self.sampler_iter = sampler_iter\n",
    "        self.getitem_func = getitem_func\n",
    "        self.collate_func = collate_func\n",
    "        self.num_threads = num_threads if num_threads is not None else max(1, os.cpu_count()-1)\n",
    "        self.cached_batch_count = cached_batch_count\n",
    "        self.sub_batch_divisor = sub_batch_divisor\n",
    "        self.debug_out_queue = debug_out_queue\n",
    "        self.sub_batch_size = 0\n",
    "\n",
    "    def finalize(self):\n",
    "        if self.num_threads:\n",
    "            # No divisor seems to show the best result from the very limited testing so far even with multiple threads ¯\\_(ツ)_/¯\n",
    "            self.sub_batch_divisor = max(1.0, self.sub_batch_divisor)\n",
    "        else:\n",
    "            self.cached_batch_count = 0\n",
    "            self.sub_batch_divisor = 1.0\n",
    "        self.sub_batch_size = math.ceil(self.sampler_iter.opts.batch_size / self.sub_batch_divisor)\n",
    "\n",
    "        return self\n",
    "\n",
    "class Collator:\n",
    "    class CachedIter:\n",
    "        def __init__(self, iter, num_groups_total):\n",
    "            self.iter = iter\n",
    "            self.num_groups_total = num_groups_total\n",
    "            self.num_groups_requested = 0\n",
    "            self.num_groups_started = 0\n",
    "            self.num_groups_finished = 0\n",
    "            self.lock = threading.Lock()\n",
    "            self.collated_queue = queue.SimpleQueue() # Not the greatest to have a queue per CachedIter, but it simplifies the DataLoader\n",
    "\n",
    "    def __init__(self, copts: COPTS):\n",
    "        self.opts = copts.finalize()\n",
    "\n",
    "        self.threads_spawned = False\n",
    "\n",
    "        self.work_queue = queue.SimpleQueue()\n",
    "        \n",
    "        self.global_lock = threading.Lock()\n",
    "        self.current_iter_serial = 0\n",
    "        self.cached_iters: dict[int, Collator.CachedIter] = {}\n",
    "\n",
    "    def start_new_iter(self):\n",
    "        if not self.threads_spawned: \n",
    "            for i in range(self.opts.num_threads): threading.Thread(target=collator_threadproc, args=(i+1, self)).start()\n",
    "            self.threads_spawned = True\n",
    "\n",
    "        self.global_lock.acquire()\n",
    "        \n",
    "        iter_serials_to_del = []\n",
    "        for iter_serial, cached_iter in self.cached_iters.items():\n",
    "            if cached_iter.num_groups_requested == cached_iter.num_groups_finished:\n",
    "                iter_serials_to_del.append(iter_serial)\n",
    "        for iter_serial in iter_serials_to_del:\n",
    "            del self.cached_iters[iter_serial]\n",
    "\n",
    "        self.current_iter_serial += 1\n",
    "        new_iter_serial = self.current_iter_serial\n",
    "        if self.opts.debug_out_queue: self.opts.debug_out_queue.put(f\"[0] New iter {new_iter_serial}\")\n",
    "\n",
    "        self.cached_iters[new_iter_serial] = self.CachedIter(iter(self.opts.sampler_iter), self.opts.sampler_iter.num_batches)\n",
    "        self.global_lock.release()\n",
    "\n",
    "        self.load_batches(new_iter_serial, self.opts.cached_batch_count + 1)\n",
    "\n",
    "        return new_iter_serial, self.cached_iters[new_iter_serial]\n",
    "    \n",
    "    def load_batches(self, iter_serial, num_batches):\n",
    "        queued_any = False\n",
    "\n",
    "        cached_iter = self.cached_iters[iter_serial]\n",
    "        cached_iter.lock.acquire()\n",
    "        new_num_groups_requested = min(cached_iter.num_groups_requested + num_batches, cached_iter.num_groups_total)\n",
    "        if new_num_groups_requested != cached_iter.num_groups_requested:\n",
    "            to_request = new_num_groups_requested-cached_iter.num_groups_requested\n",
    "            self.work_queue.put(WorkItemLoadBatches(iter_serial, to_request))\n",
    "            cached_iter.num_groups_requested = new_num_groups_requested\n",
    "            if self.opts.debug_out_queue: self.opts.debug_out_queue.put(f\"[0] Requesting {to_request} batches for iter {iter_serial}\")\n",
    "            queued_any = True\n",
    "        cached_iter.lock.release()\n",
    "\n",
    "        if self.opts.num_threads == 0 and queued_any:\n",
    "            collator_threadproc(0, self)\n",
    "\n",
    "    def shutdown(self):\n",
    "        if self.threads_spawned:\n",
    "            for _ in range(self.opts.num_threads): self.work_queue.put(WorkItem(WORK_TYPE_SHUTDOWN))\n",
    "\n",
    "class WorkGroup:\n",
    "    def __init__(self, iter_serial, batch_serial, num_total):\n",
    "        self.iter_serial = iter_serial\n",
    "        self.batch_serial = batch_serial\n",
    "        self.num_total = num_total\n",
    "        self.gi_work_array: list[WorkItemGetItems] = []\n",
    "\n",
    "        self.num_done_lock = threading.Lock()\n",
    "        self.num_done = 0\n",
    "\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}(iter_serial={self.iter_serial}, batch_serial={self.batch_serial}, \"\\\n",
    "                                    f\"num_done={self.num_done}, num_total={self.num_total})\"\n",
    "\n",
    "class CollatedResult:\n",
    "    def __init__(self, iter_serial, batch_serial, result):\n",
    "        self.iter_serial = iter_serial # Don't really need to store the iter_serial in results anymore since every CachedIter has it's own queue now\n",
    "        self.batch_serial = batch_serial\n",
    "        self.result = result\n",
    "\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}({str(vars(self))})\"\n",
    "\n",
    "def collator_threadproc(thread_id: int, ctx: Collator):\n",
    "    work_queue = ctx.work_queue\n",
    "    global_lock = ctx.global_lock\n",
    "    sub_batch_size = ctx.opts.sub_batch_size\n",
    "    cached_iters = ctx.cached_iters\n",
    "    getitem_func = ctx.opts.getitem_func\n",
    "    collate_func = ctx.opts.collate_func\n",
    "    debug_out_queue = ctx.opts.debug_out_queue\n",
    "\n",
    "    if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Started\")\n",
    "\n",
    "    while 1:\n",
    "        work: WorkItem = work_queue.get()\n",
    "        if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Got {work}\")\n",
    "        work_type = work.type\n",
    "        \n",
    "        if work_type == WORK_TYPE_LOAD_BATCHES:\n",
    "            lb_work: WorkItemLoadBatches = work\n",
    "            \n",
    "            global_lock.acquire()\n",
    "            cached_iter = cached_iters[lb_work.iter_serial]\n",
    "            global_lock.release()\n",
    "            \n",
    "            cached_iter.lock.acquire()\n",
    "            read_num_groups = cached_iter.num_groups_started\n",
    "            \n",
    "            cached_iter.num_groups_started += lb_work.num_batches\n",
    "            array_of_batch_indices = list(itertools.islice(cached_iter.iter, lb_work.num_batches))\n",
    "            cached_iter.lock.release()\n",
    "\n",
    "            batches_spawned = 0\n",
    "            for batch_indices in array_of_batch_indices:\n",
    "                sub_batches = mins.chunkify(batch_indices, sub_batch_size)\n",
    "                assert len(sub_batches)\n",
    "\n",
    "                batch_serial = read_num_groups + batches_spawned\n",
    "                work_group = WorkGroup(lb_work.iter_serial, batch_serial + 1, len(sub_batches))\n",
    "                for sub_batch in sub_batches:\n",
    "                    work_queue.put(WorkItemGetItems(work_group, sub_batch))\n",
    "                    if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Queued {work_group.gi_work_array[-1]}\")\n",
    "\n",
    "                batches_spawned += 1\n",
    "\n",
    "\n",
    "        elif work_type == WORK_TYPE_GET_ITEMS:\n",
    "            gi_work: WorkItemGetItems = work\n",
    "            work_group = gi_work.group\n",
    "            group_iter_serial = work_group.iter_serial\n",
    "\n",
    "            gi_work.results = getitem_func(gi_work.indices)\n",
    "\n",
    "            work_group.num_done_lock.acquire()\n",
    "            work_group.num_done += 1\n",
    "            work_group.num_done_lock.release()\n",
    "            assert work_group.num_done <= work_group.num_total\n",
    "            \n",
    "            if work_group.num_done == work_group.num_total:\n",
    "                assert work_group.num_done == len(work_group.gi_work_array)\n",
    "                if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Completed group {work_group}\")\n",
    "\n",
    "                global_lock.acquire()\n",
    "                cached_iter = cached_iters[group_iter_serial]\n",
    "                global_lock.release()\n",
    "                \n",
    "                cached_iter.lock.acquire()\n",
    "                cached_iter.num_groups_finished += 1\n",
    "                is_last_group = cached_iter.num_groups_finished == cached_iter.num_groups_total\n",
    "                cached_iter.lock.release()\n",
    "\n",
    "                collated = collate_func([gi_work.results for gi_work in work_group.gi_work_array])\n",
    "                cached_iter.collated_queue.put(CollatedResult(group_iter_serial, work_group.batch_serial, collated))\n",
    "\n",
    "                if is_last_group:\n",
    "                    if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Completed iter {group_iter_serial}\")\n",
    "                    cached_iter.collated_queue.put(CollatedResult(group_iter_serial, 0, None))\n",
    "\n",
    "                if thread_id == 0:\n",
    "                    break\n",
    "                \n",
    "\n",
    "        elif work_type == WORK_TYPE_SHUTDOWN:\n",
    "            break\n",
    "\n",
    "        else: assert False\n",
    "\n",
    "    if debug_out_queue: debug_out_queue.put(f\"[{thread_id}] Exiting\")\n",
    "\n",
    "\n",
    "class DataLoaderIter:\n",
    "    def __init__(self, collator, iter_serial, collated_queue):\n",
    "        self.collator = collator\n",
    "        self.iter_serial = iter_serial\n",
    "        self.collated_queue = collated_queue\n",
    "        \n",
    "        self.buffer: list[CollatedResult] = []\n",
    "        self.next_batch_index = 1\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while 1:\n",
    "            collated: CollatedResult = self.collated_queue.get()\n",
    "            assert collated.iter_serial == self.iter_serial\n",
    "\n",
    "            if not collated.batch_serial:\n",
    "                break\n",
    "\n",
    "            self.buffer.append(collated)\n",
    "\n",
    "            found_i = next((i for i, x in enumerate(self.buffer) if x.batch_serial == self.next_batch_index), -1)\n",
    "            if found_i != -1:\n",
    "                to_yield = self.buffer[found_i].result\n",
    "                self.buffer[found_i] = self.buffer[-1]\n",
    "                self.buffer.pop()\n",
    "                self.next_batch_index += 1\n",
    "                \n",
    "                yield to_yield\n",
    "                self.collator.load_batches(self.iter_serial, 1)\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, ds, copts: COPTS):\n",
    "        self.ds = ds\n",
    "        self.collator = Collator(copts)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.collator.shutdown()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        current_iter, cached_iter = self.collator.start_new_iter()\n",
    "        return iter(DataLoaderIter(self.collator, current_iter, cached_iter.collated_queue))\n",
    "    \n",
    "    @classmethod\n",
    "    def simple(cls, simple_ds: minds.SimpleDataset, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):\n",
    "        sampler_iter_opts = sampler_iter_opts or mins.SIO()\n",
    "        collator_opts = collator_opts or COPTS()\n",
    "\n",
    "        collator_opts.sampler_iter = mins.Sampler(len(simple_ds)).iter(sampler_iter_opts)\n",
    "        collator_opts.getitem_func = simple_ds.__getitem__\n",
    "        collator_opts.collate_func = simple_collate_func\n",
    "        return cls(simple_ds, collator_opts)\n",
    "\n",
    "    @classmethod\n",
    "    def hf(cls, hf_ds: hfds.Dataset, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):\n",
    "        assert type(hf_ds) is hfds.Dataset, f\"Dataset expected, not {type(hf_ds).__name__}\"\n",
    "        sampler_iter_opts = sampler_iter_opts or mins.SIO()\n",
    "        collator_opts = collator_opts or COPTS()\n",
    "\n",
    "        collator_opts.sampler_iter = mins.Sampler(len(hf_ds)).iter(sampler_iter_opts)\n",
    "        collator_opts.getitem_func = hf_ds.__getitem__\n",
    "        collator_opts.collate_func = HFCollate(hf_ds).__call__\n",
    "        return cls(hf_ds, collator_opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ([0, 1, 2, 3, 4], [0, -1, -2, -3, -4])\n",
      "2 ([0, 1, 2, 3, 4], [0, -1, -2, -3, -4])\n",
      "1 ([5, 6, 7, 8, 9], [-5, -6, -7, -8, -9])\n",
      "1 ([10, 2, 10, 7, 7], [-10, -2, -10, -7, -7])\n",
      "2 ([5, 6, 7, 8, 9], [-5, -6, -7, -8, -9])\n",
      "2 ([10, 9, 10, 9, 10], [-10, -9, -10, -9, -10])\n",
      "\n",
      "[0, 1, 2, 3, 4] [0, -1, -2, -3, -4]\n",
      "[5, 6, 7, 8, 9] [-5, -6, -7, -8, -9]\n",
      "[10, 3, 0, 10, 5] [-10, -3, 0, -10, -5]\n",
      "agane\n",
      "[0, 1, 2, 3, 4] [0, -1, -2, -3, -4]\n",
      "[5, 6, 7, 8, 9] [-5, -6, -7, -8, -9]\n",
      "[10, 8, 0, 10, 3] [-10, -8, 0, -10, -3]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "def getitem_func(indices):\n",
    "    #time.sleep(random.random())\n",
    "    return [[i, -i] for i in indices]\n",
    "\n",
    "dl = DataLoader(None, COPTS(mins.Sampler(11).iter(mins.SIO(5)), getitem_func, simple_collate_func, num_threads=4, cached_batch_count=1))\n",
    "\n",
    "it1 = iter(dl)\n",
    "it2 = iter(dl)\n",
    "print(1, next(it1))\n",
    "print(2, next(it2))\n",
    "print(1, next(it1))\n",
    "print(1, next(it1))\n",
    "print(2, next(it2))\n",
    "print(2, next(it2))\n",
    "print()\n",
    "\n",
    "for xb, yb in dl:\n",
    "    print(xb, yb)\n",
    "print(\"agane\")\n",
    "for xb, yb in dl:\n",
    "    print(xb, yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SimpleDataset(len=100, xs=int, ys=int),\n",
       " {'opts': <__main__.COPTS at 0x7f4e2c56cad0>,\n",
       "  'threads_spawned': False,\n",
       "  'work_queue': <_queue.SimpleQueue at 0x7f4debd6bba0>,\n",
       "  'global_lock': <unlocked _thread.lock object at 0x7f4e2c350980>,\n",
       "  'current_iter_serial': 0,\n",
       "  'cached_iters': {}},\n",
       " {'sampler_iter': <minai.sampler.SamplerIter at 0x7f4e0c69bad0>,\n",
       "  'getitem_func': <bound method SimpleDataset.__getitem__ of SimpleDataset(len=100, xs=int, ys=int)>,\n",
       "  'collate_func': <function __main__.simple_collate_func(array_of_results)>,\n",
       "  'num_threads': 14,\n",
       "  'cached_batch_count': 1,\n",
       "  'sub_batch_divisor': 1.0,\n",
       "  'debug_out_queue': None,\n",
       "  'sub_batch_size': 16})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = minds.SimpleDataset(list(range(100)), list(range(0, -100, -1)))\n",
    "dl = DataLoader.simple(ds, mins.SIO(16))\n",
    "ds, vars(dl.collator), vars(dl.collator.opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15])\n",
      "([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31])\n",
      "([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15])\n",
      "([16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], [-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31])\n"
     ]
    }
   ],
   "source": [
    "it = iter(dl)\n",
    "print(next(it))\n",
    "print(next(it))\n",
    "\n",
    "it = iter(dl)\n",
    "print(next(it))\n",
    "print(next(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fashion_mnist (/home/nblzv/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/0a671f063342996f19779d38c0ab4abef9c64f757b35af8134b331c294d7ba48)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7bcab5c9c49480297b40829a9daf9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsd = minds.hf_load(minds.HF_DATASETS.FASHION_MNIST)\n",
    "dst = dsd[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HFCollate(features=('image', 'label'))\n",
      "[[<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F4E24134050>, <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F4E24135B90>, <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28 at 0x7F4E24134810>], [9, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "c = HFCollate(dst)\n",
    "print(c)\n",
    "print(c([dst[[0, 1]], dst[[2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15]\n",
      "[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31] [-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31]\n",
      "[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47] [-32, -33, -34, -35, -36, -37, -38, -39, -40, -41, -42, -43, -44, -45, -46, -47]\n",
      "[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63] [-48, -49, -50, -51, -52, -53, -54, -55, -56, -57, -58, -59, -60, -61, -62, -63]\n",
      "[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79] [-64, -65, -66, -67, -68, -69, -70, -71, -72, -73, -74, -75, -76, -77, -78, -79]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95] [-80, -81, -82, -83, -84, -85, -86, -87, -88, -89, -90, -91, -92, -93, -94, -95]\n",
      "[96, 97, 98, 99, 98, 63, 80, 96, 94, 99, 60, 53, 64, 56, 15, 10] [-96, -97, -98, -99, -98, -63, -80, -96, -94, -99, -60, -53, -64, -56, -15, -10]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15] [0, -1, -2, -3, -4, -5, -6, -7, -8, -9, -10, -11, -12, -13, -14, -15]\n",
      "[16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31] [-16, -17, -18, -19, -20, -21, -22, -23, -24, -25, -26, -27, -28, -29, -30, -31]\n",
      "[32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47] [-32, -33, -34, -35, -36, -37, -38, -39, -40, -41, -42, -43, -44, -45, -46, -47]\n",
      "[48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63] [-48, -49, -50, -51, -52, -53, -54, -55, -56, -57, -58, -59, -60, -61, -62, -63]\n",
      "[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79] [-64, -65, -66, -67, -68, -69, -70, -71, -72, -73, -74, -75, -76, -77, -78, -79]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95] [-80, -81, -82, -83, -84, -85, -86, -87, -88, -89, -90, -91, -92, -93, -94, -95]\n",
      "[96, 97, 98, 99, 43, 82, 98, 29, 45, 85, 54, 3, 33, 66, 86, 76] [-96, -97, -98, -99, -43, -82, -98, -29, -45, -85, -54, -3, -33, -66, -86, -76]\n"
     ]
    }
   ],
   "source": [
    "for _ in range(2):\n",
    "    for xs, ys in dl:\n",
    "        print(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[<PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>,\n",
       "  <PIL.PngImagePlugin.PngImageFile image mode=L size=28x28>],\n",
       " [9, 0, 0, 3, 0, 2, 7, 2, 5]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader.hf(dst, mins.SIO(9, False))\n",
    "next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "class HFTransform:\n",
    "    def __init__(self, features, transform, **extra_args):\n",
    "        assert type(features) is hfds.features.features.Features\n",
    "        self.features = tuple(features)\n",
    "        self.transform = transform\n",
    "\n",
    "        for k, v in extra_args.items():\n",
    "            super().__setattr__(k ,v)\n",
    "\n",
    "    def __call__(self, results):\n",
    "        return self.transform(self, results)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"HFTransform(features={list(self.features)})\"\n",
    "\n",
    "    @classmethod\n",
    "    def ff_img_to_tensor(cls, features): # first_feature\n",
    "        def tf(ctx: HFTransform, results):\n",
    "            xs = results[ctx.features[0]]\n",
    "            for i in range(len(xs)):\n",
    "                xs[i] = TF.to_tensor(xs[i])\n",
    "            return results\n",
    "        \n",
    "        return cls(features, tf)\n",
    "    \n",
    "    @classmethod\n",
    "    def ff_img_decode_to_tensor(cls, features, half=False): # first_feature\n",
    "        def tf(ctx: HFTransform, results):\n",
    "            xs = results[ctx.features[0]]\n",
    "            for i in range(len(xs)):\n",
    "                raw = torch.frombuffer(xs[i][\"bytes\"], dtype=torch.uint8)\n",
    "                decoded = TFIO.decode_image(raw)\n",
    "                if ctx.half: decoded = decode.half()\n",
    "                else: decoded = decoded.float()\n",
    "                xs[i] = decoded / 255.0\n",
    "            return results\n",
    "        \n",
    "        return cls(features, tf, half=half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "def first(iterable):\n",
    "    return next(iter(iterable))\n",
    "\n",
    "def first_value(iterable):\n",
    "    return next(iter(iterable.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def to_tensor_transform(ctx, results):\n",
    "    xs = results[ctx.features[0]]\n",
    "    for i in range(len(xs)):\n",
    "        xs[i] = TF.to_tensor(xs[i])\n",
    "    return results\n",
    "\n",
    "thds = dst.with_transform(HFTransform(dst.features, to_tensor_transform))\n",
    "dl = DataLoader.hf(thds, mins.SIO(5, False))\n",
    "first(dl)[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefetch 0\n",
      "12.8 ms ± 682 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "20.7 ms ± 798 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "28.6 ms ± 685 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "prefetch 1\n",
      "34.7 ms ± 2.15 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "51.8 ms ± 15.5 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "58.6 ms ± 9.21 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "tdst = dst.with_transform(HFTransform.ff_img_to_tensor(dst.features))\n",
    "def bench(cached_batch_count, next_count):\n",
    "    dl = DataLoader.hf(tdst, mins.SIO(64), collator_opts=COPTS(cached_batch_count=cached_batch_count))\n",
    "    it = iter(dl)\n",
    "    for _ in range(next_count): next(it)\n",
    "print(\"prefetch 0\")\n",
    "%timeit -r 10 -n 1 bench(0, 1);\n",
    "%timeit -r 10 -n 1 bench(0, 2);\n",
    "%timeit -r 10 -n 1 bench(0, 3);\n",
    "print(\"prefetch 1\")\n",
    "%timeit -r 10 -n 1 bench(1, 1); # For some reason this shows as slower when it is in fact pretty much the same time for the first batch to complete\n",
    "%timeit -r 10 -n 1 bench(1, 2);\n",
    "%timeit -r 10 -n 1 bench(1, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefetch 0\n",
      "20.5 ms ± 784 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "25.4 ms ± 1.14 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "29.4 ms ± 828 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "prefetch 1\n",
      "31.3 ms ± 930 µs per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "40.2 ms ± 7.95 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n",
      "44.5 ms ± 2.45 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "tdst = dst.with_transform(HFTransform.ff_img_decode_to_tensor(dst.features)).cast_column(\"image\", hfds.Image(decode=False))\n",
    "def bench(cached_batch_count, next_count):\n",
    "    dl = DataLoader.hf(tdst, mins.SIO(64), collator_opts=COPTS(cached_batch_count=cached_batch_count))\n",
    "    it = iter(dl)\n",
    "    for _ in range(next_count): \n",
    "        next(it)\n",
    "print(\"prefetch 0\")\n",
    "%timeit -r 10 -n 1 bench(0, 1);\n",
    "%timeit -r 10 -n 1 bench(0, 2);\n",
    "%timeit -r 10 -n 1 bench(0, 3);\n",
    "print(\"prefetch 1\")\n",
    "%timeit -r 10 -n 1 bench(1, 1); # For some reason this shows as slower when it is in fact pretty much the same time for the first batch to complete\n",
    "%timeit -r 10 -n 1 bench(1, 2);\n",
    "%timeit -r 10 -n 1 bench(1, 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#e\n",
    "class DataLoaderDict(dict):\n",
    "    def __init__(self, dataloaders_dict: dict[str, DataLoader]):\n",
    "        super().__init__(dataloaders_dict)\n",
    "\n",
    "    def __getitem__(self, key) -> DataLoader:\n",
    "        return super().__getitem__(key)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"DataLoaders({super().__repr__()})\"\n",
    "\n",
    "    @classmethod\n",
    "    def hf(cls, dsd: hfds.DatasetDict, sampler_iter_opts: mins.SIO = None, collator_opts: COPTS = None):\n",
    "        dls = {k: DataLoader.hf(dsd[k], sampler_iter_opts, collator_opts) for k in dsd}\n",
    "        return cls(dls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataLoaders({'train': <__main__.DataLoader object at 0x7f4e0d0cf790>, 'test': <__main__.DataLoader object at 0x7f4e0d0cf9d0>})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls = DataLoaderDict.hf(dsd)\n",
    "dls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing minai_nbs/datasets.ipynb -> minai/minai/datasets.py  |  same contents, skipping, took 0.001s\n",
      "Processing minai_nbs/sampler.ipynb -> minai/minai/sampler.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/setup+template.py -> minai/setup.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/__init__+template.py -> minai/minai/__init__.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/plot.ipynb -> minai/minai/plot.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/mintils.py -> minai/minai/mintils.py  |  same contents, skipping, took 0.000s\n",
      "Processing minai_nbs/data.ipynb -> minai/minai/data.py  |  6 cells exported, took 0.001s \n",
      "\n",
      "All done... took 0.004s\n",
      "  lib_name: minai\n",
      "  author: nblzv\n",
      "  version: 0.1.1\n"
     ]
    }
   ],
   "source": [
    "import z_export\n",
    "z_export.export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
